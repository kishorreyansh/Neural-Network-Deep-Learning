# -*- coding: utf-8 -*-
"""AutoEncodersDenoisening.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1egPidiGTOeSeDKs2RQgjeF3kJvwSIlX1

**ADDING THE HIDDEN LAYER TO THE AUTOENCODER AND VISUALIZING THE DATA**
"""

from keras.layers import Input, Dense
from keras.models import Model
import matplotlib.pyplot as plt

# Define the size of encoded representations and the additional hidden layer size
encoding_dim = 32
hidden_dim = 64

# Input placeholder
input_img = Input(shape=(784,))

# First Encoding Layer
encoded1 = Dense(hidden_dim, activation='relu')(input_img)
# Second Encoding Layer
encoded2 = Dense(encoding_dim, activation='relu')(encoded1)

# First Decoding layer
decoded1 = Dense(hidden_dim, activation='relu')(encoded2)
# Second Decoding layer
decoded = Dense(784, activation='relu')(decoded1)

# Creating the autoencoder model
autoencoder = Model(input_img, decoded)

# Compile the autoencoder model
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# Loading and preprocessing the data
from keras.datasets import fashion_mnist
import numpy as np

(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

# Train the autoencoder
history = autoencoder.fit(x_train, x_train, epochs=25, batch_size=256, shuffle=True, validation_data=(x_test, x_test))

# Predict and visualize one of the reconstructed test data
decoded_imgs = autoencoder.predict(x_test)

n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    # Display original images
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28),cmap='gray')
    plt.title("Original")
    plt.axis('off')

    # Display reconstructed images
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28),cmap='gray')
    plt.title("Reconstructed")
    plt.axis('off')
plt.show()

# Visualize the loss and accuracy
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

"""**ADDING HIDDEN LAYER TO THE DENOISENING AUTOENCODER AND VISUALIZING THE DATA**"""

from keras.layers import Input, Dense
from keras.models import Model
import matplotlib.pyplot as plt
import numpy as np

encoding_dim = 32
hidden_dim = 64

# Input placeholder for noisy data
input_img = Input(shape=(784,))

# First Encoding Layer
encoded1 = Dense(hidden_dim, activation='relu')(input_img)

# Second Encoding Layer
encoded2 = Dense(encoding_dim, activation='relu')(encoded1)

# First Decoding layer
decoded1 = Dense(hidden_dim, activation='relu')(encoded2)

# Second Decoding layer
decoded2 = Dense(784, activation='sigmoid')(decoded1)

# Creating the denoising autoencoder model
autoencoder = Model(input_img, decoded2)

# Compiling the denoising autoencoder model
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# Loading and Preprocessing the data
from keras.datasets import fashion_mnist

(x_train, _), (x_test, _) = fashion_mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

# Introducing Noise
noise_factor = 0.5
x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)
x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)

# Train the Denoising Autoencoder
history = autoencoder.fit(x_train_noisy, x_train, epochs=20, batch_size=256, shuffle=True, validation_data=(x_test_noisy, x_test_noisy))

# Predict and visualize one of the reconstructed test data
decoded_imgs = autoencoder.predict(x_test_noisy)

n = 10
plt.figure(figsize=(10, 30))
for i in range(n):
    # Display Noisy Images
    ax = plt.subplot(n, 3,i*3 + 1)
    plt.imshow(x_test_noisy[i].reshape(28, 28),cmap='gray')
    plt.title("Noisy")
    plt.axis('off')

    # Display Original Images
    ax = plt.subplot(n, 3, i*3 + 2)
    plt.imshow(x_test[i].reshape(28, 28),cmap='gray')
    plt.title("Original")
    plt.axis('off')

    # Display Reconstructed Images
    ax = plt.subplot(n,3, i*3 + 3)
    plt.imshow(decoded_imgs[i].reshape(28, 28),cmap='gray')
    plt.title("Reconstructed")
    plt.axis('off')

plt.show()

# Visualize the loss and accuracy
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()